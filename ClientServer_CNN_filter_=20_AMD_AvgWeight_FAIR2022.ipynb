{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekkf6NJ_S4d6",
        "outputId": "e46ea144-7f68-49c1-e125-fc48f8f632d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0vX9HCXxLqC"
      },
      "source": [
        "Setup model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHaomBqoxJTE"
      },
      "source": [
        "# baseline cnn model for mnist\n",
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import numpy as np\n",
        "import timeit\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "cols = [2] + list(range(16386, 18264))\n",
        "def load_top3_dataset(n):\n",
        "\tfile0_df = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/10set (cắt file >=20)/AMD_Benign(tổng hợp >= 20)/file-{}.csv'.format(n), header=None, skiprows=1, usecols=cols)\n",
        "\toutput = np.array([]).reshape(0, 1879)\n",
        "\tfor index, row in file0_df.iterrows():\n",
        "\t\tr = np.array([row])                                                                      \n",
        "\t\toutput = np.concatenate((output, r), axis=0)\n",
        "\treturn output[:, 0], output[:, 1:]\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\t# reshape dataset to have a single channel\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\t# model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(44, 44, 1)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(228, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.01, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# # load dataset\n",
        "# trainX, trainY, test_X, test_Y = load_dataset()\n",
        "# # prepare pixel data\n",
        "# trainX, testX = prep_pixels(trainX, test_X)\n",
        "# # evaluate model\n",
        "# test_X, valX, test_Y, valY = train_test_split(test_X, test_Y, test_size=0.5)\n",
        "# trainX, trainX1, trainY, trainY1 = train_test_split(trainX, trainY, test_size=0.3)\n",
        "# trainX, trainX2, trainY, trainY2 = train_test_split(trainX, trainY, test_size=0.4)\n",
        "# trainX, trainX3, trainY, trainY3 = train_test_split(trainX, trainY, test_size=0.5)\n",
        "# trainX1, trainX4, trainY1, trainY4 = train_test_split(trainX1, trainY1, test_size=0.35)\n",
        "# trainX1, trainX5, trainY1, trainY5 = train_test_split(trainX1, trainY1, test_size=0.25)\n",
        "# trainX2, trainX6, trainY2, trainY6 = train_test_split(trainX2, trainY2, test_size=0.45)\n",
        "# trainX4, trainX7, trainY4, trainY7 = train_test_split(trainX4, trainY4, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QskrHouqehNt"
      },
      "source": [
        "file0_y, file0_x = load_top3_dataset(0)\n",
        "file1_y, file1_x = load_top3_dataset(1)\n",
        "file2_y, file2_x = load_top3_dataset(2)\n",
        "file3_y, file3_x = load_top3_dataset(3)\n",
        "file4_y, file4_x = load_top3_dataset(4)\n",
        "file5_y, file5_x = load_top3_dataset(5)\n",
        "file6_y, file6_x = load_top3_dataset(6)\n",
        "file7_y, file7_x = load_top3_dataset(7)\n",
        "file8_y, file8_x = load_top3_dataset(8)\n",
        "file9_y, file9_x = load_top3_dataset(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kiểm tra nhãn của các file => có 38 nhãn là là tổng hợp các nhãn có từ 20file trở lên"
      ],
      "metadata": {
        "id": "6Qz6oDbwSgxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(file0_y, return_counts=True)\n",
        "print(\"LABEL TRAIN: \", len(unique))\n",
        "unique, counts = np.unique(file1_y, return_counts=True)\n",
        "print(\"LABEL TRAIN1: \", len(unique))\n",
        "unique, counts = np.unique(file2_y, return_counts=True)\n",
        "print(\"LABEL TRAIN2: \", len(unique))\n",
        "unique, counts = np.unique(file3_y, return_counts=True)\n",
        "print(\"LABEL TRAIN3: \", len(unique))\n",
        "unique, counts = np.unique(file4_y, return_counts=True)\n",
        "print(\"LABEL TRAIN4: \", len(unique))\n",
        "unique, counts = np.unique(file5_y, return_counts=True)\n",
        "print(\"LABEL TRAIN5: \", len(unique))\n",
        "unique, counts = np.unique(file6_y, return_counts=True)\n",
        "print(\"LABEL TRAIN6: \", len(unique))\n",
        "unique, counts = np.unique(file7_y, return_counts=True)\n",
        "print(\"LABEL TRAIN7: \", len(unique))\n",
        "unique, counts = np.unique(file8_y, return_counts=True)\n",
        "print(\"LABEL TRAIN8: \", len(unique))\n",
        "unique, counts = np.unique(file9_y, return_counts=True)\n",
        "print(\"LABEL TRAIN9: \", len(unique))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYOf7CGZSjW8",
        "outputId": "18662f2f-8542-4d9f-c257-095d14f5b857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL TRAIN:  38\n",
            "LABEL TRAIN1:  38\n",
            "LABEL TRAIN2:  38\n",
            "LABEL TRAIN3:  38\n",
            "LABEL TRAIN4:  38\n",
            "LABEL TRAIN5:  38\n",
            "LABEL TRAIN6:  38\n",
            "LABEL TRAIN7:  38\n",
            "LABEL TRAIN8:  38\n",
            "LABEL TRAIN9:  38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pgh153_ekwF"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "trainY = to_categorical(file0_y, 228)\n",
        "trainY1 = to_categorical(file1_y, 228)\n",
        "trainY2 = to_categorical(file2_y, 228)\n",
        "trainY3 = to_categorical(file3_y, 228)\n",
        "trainY4 = to_categorical(file4_y, 228)\n",
        "trainY5 = to_categorical(file5_y, 228)\n",
        "trainY6 = to_categorical(file6_y, 228)\n",
        "trainY7 = to_categorical(file7_y, 228)\n",
        "valY = to_categorical(file8_y, 228)\n",
        "test_Y = to_categorical(file9_y, 228)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKPvNirpemIE"
      },
      "source": [
        "trainX = file0_x\n",
        "trainX1 = file1_x\n",
        "trainX2 = file2_x\n",
        "trainX3 = file3_x\n",
        "trainX4 = file4_x\n",
        "trainX5 = file5_x\n",
        "trainX6 = file6_x\n",
        "trainX7 = file7_x\n",
        "valX = file8_x\n",
        "test_X = file9_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9M0OzMPenSV"
      },
      "source": [
        "trainX = np.concatenate((trainX,np.zeros((trainX.shape[0], 58))),1)\n",
        "trainX1 = np.concatenate((trainX1,np.zeros((trainX1.shape[0], 58))),1)\n",
        "trainX2 = np.concatenate((trainX2,np.zeros((trainX2.shape[0], 58))),1)\n",
        "trainX3 = np.concatenate((trainX3,np.zeros((trainX3.shape[0], 58))),1)\n",
        "trainX4 = np.concatenate((trainX4,np.zeros((trainX4.shape[0], 58))),1)\n",
        "trainX5 = np.concatenate((trainX5,np.zeros((trainX5.shape[0], 58))),1)\n",
        "trainX6 = np.concatenate((trainX6,np.zeros((trainX6.shape[0], 58))),1)\n",
        "trainX7 = np.concatenate((trainX7,np.zeros((trainX7.shape[0], 58))),1)\n",
        "valX = np.concatenate((valX,np.zeros((valX.shape[0], 58))),1)\n",
        "test_X = np.concatenate((test_X,np.zeros((test_X.shape[0], 58))),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAAut4COeomN"
      },
      "source": [
        "trainX = trainX.reshape(trainX.shape[0],44, 44, 1)\n",
        "trainX1 = trainX1.reshape(trainX1.shape[0],44, 44, 1)\n",
        "trainX2 = trainX2.reshape(trainX2.shape[0],44, 44, 1)\n",
        "trainX3 = trainX3.reshape(trainX3.shape[0],44, 44, 1)\n",
        "trainX4 = trainX4.reshape(trainX4.shape[0],44, 44, 1)\n",
        "trainX5 = trainX5.reshape(trainX5.shape[0],44, 44, 1)\n",
        "trainX6 = trainX6.reshape(trainX6.shape[0],44, 44, 1)\n",
        "trainX7 = trainX7.reshape(trainX7.shape[0],44, 44, 1)\n",
        "valX = valX.reshape(valX.shape[0],44, 44, 1)\n",
        "test_X = test_X.reshape(test_X.shape[0],44, 44, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6EQgfqFqFlE"
      },
      "source": [
        "Check shape of input dataset. Tổng 10set có 26,029 file. số đặc trưng 1878."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7xgj_vGgawN",
        "outputId": "3f15227c-b49a-4bab-d496-39e832da51b6"
      },
      "source": [
        "print(trainX.shape)\n",
        "print(trainX1.shape)\n",
        "print(trainX2.shape)\n",
        "print(trainX3.shape)\n",
        "print(trainX4.shape)\n",
        "print(trainX5.shape)\n",
        "print(trainX6.shape)\n",
        "print(trainX7.shape)\n",
        "print(valX.shape)\n",
        "print(test_X.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2558, 44, 44, 1)\n",
            "(2705, 44, 44, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHStyegMNFFy"
      },
      "source": [
        "Step 1: \n",
        "    for 1/10 dataset train for server (sv)\n",
        "    for 1/10 dataset train for client 1 (c1)\n",
        "    for 1/10 dataset train for client 2 (c2)\n",
        "    for 1/10 dataset train for client 3 (c3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlS-zCgh-EKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3827f75-81a1-49c8-85b5-5a8ed0fa7706"
      },
      "source": [
        "c1 = define_model()\n",
        "c2 = define_model()\n",
        "c3 = define_model()\n",
        "sv = define_model()\n",
        "\n",
        "start = timeit.default_timer()\n",
        "sv.fit(trainX, trainY, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('Server: {}'.format(end-start))\n",
        "\n",
        "start = timeit.default_timer()\n",
        "c1.fit(trainX1, trainY1, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C1: {}'.format(end-start))\n",
        "\n",
        "start = timeit.default_timer()\n",
        "c2.fit(trainX2, trainY2, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C2: {}'.format(end-start))\n",
        "\n",
        "start = timeit.default_timer()\n",
        "c3.fit(trainX3, trainY3, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C3: {}'.format(end-start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server: 20.900330079000014\n",
            "C1: 10.619410666000022\n",
            "C2: 10.612042419000034\n",
            "C3: 10.636359954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-PNQFLkNmFD"
      },
      "source": [
        "Test Step 1: Test model in server and clients (c1, c2, c3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r38KhdGcgsnD",
        "outputId": "75a11897-45eb-4122-ca0c-c40638b38f67"
      },
      "source": [
        "\n",
        "print(\"==============train result=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C1: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c2.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C2: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c3.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C3: ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============train result=============\n",
            "Server:  0.9471349353049907\n",
            "C1:  0.9482439926062847\n",
            "C2:  0.9489833641404806\n",
            "C3:  0.9449168207024029\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SHoaLz9OKRD"
      },
      "source": [
        "Step 2: update weight_avg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IliZG0RTSdTL",
        "outputId": "f51b58e9-eca3-46b7-edf7-0b8dfe01a541"
      },
      "source": [
        "avg = (np.array(c1.get_weights()) + np.array(c2.get_weights()) + np.array(c3.get_weights()) + np.array(sv.get_weights())) / 4\n",
        "c1.set_weights(avg)\n",
        "c2.set_weights(avg)\n",
        "c3.set_weights(avg)\n",
        "sv.set_weights(avg)\n",
        "print(\"==============Result with weight_Avg=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Client 1 (c1): ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Result with weight_Avg=============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server:  0.38964879852125694\n",
            "Client 1 (c1):  0.38964879852125694\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqM-YblUN9OS"
      },
      "source": [
        "Step 3: train client 1 and client 2 with new dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0amqONBTccC",
        "outputId": "49447da9-91b2-47ef-e82f-f4ef4d960659"
      },
      "source": [
        "start = timeit.default_timer()\n",
        "c1.fit(trainX4, trainY4, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C1: {}'.format(end-start))\n",
        "\n",
        "start = timeit.default_timer()\n",
        "c2.fit(trainX5, trainY5, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C2: {}'.format(end-start))\n",
        "\n",
        "print(\"==============train result=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C1: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c2.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C2: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c3.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C3: ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C1: 10.28552880199993\n",
            "C2: 9.836548838999988\n",
            "==============train result=============\n",
            "Server:  0.38964879852125694\n",
            "C1:  0.9489833641404806\n",
            "C2:  0.9463955637707948\n",
            "C3:  0.38964879852125694\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ijfwxVXO0iI"
      },
      "source": [
        "Step 4: update weight_avg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSIm6rBRYKDB",
        "outputId": "767e7827-be3b-4cd9-e736-9786ad9f69c1"
      },
      "source": [
        "avg = (np.array(c1.get_weights()) + np.array(c2.get_weights()) + np.array(c3.get_weights()) + np.array(sv.get_weights())) / 4\n",
        "c1.set_weights(avg)\n",
        "c2.set_weights(avg)\n",
        "c3.set_weights(avg)\n",
        "sv.set_weights(avg)\n",
        "print(\"==============Result Avg=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Client 1: ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Result Avg=============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server:  0.855452865064695\n",
            "Client 1:  0.855452865064695\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPdYsF4vPFQC"
      },
      "source": [
        "Step 5: Next, train client 1 and client 2, no train server and client 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hQZVezQUGu5",
        "outputId": "7ca85cb4-9bba-4966-c04c-2674b8bbcfa5"
      },
      "source": [
        "start = timeit.default_timer()\n",
        "c1.fit(trainX6, trainY6, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C1: {}'.format(end-start))\n",
        "\n",
        "start = timeit.default_timer()\n",
        "c2.fit(trainX7, trainY7, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "end = timeit.default_timer()\n",
        "print('C2: {}'.format(end-start))\n",
        "\n",
        "print(\"==============Result train=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C1: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c2.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C2: ', accuracy_score(y_pred1, y_pred))\n",
        "\n",
        "y_pred = c3.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('C3: ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C1: 9.718953374999955\n",
            "C2: 9.732741863000001\n",
            "==============Result train=============\n",
            "Server:  0.855452865064695\n",
            "C1:  0.9582255083179297\n",
            "C2:  0.9560073937153419\n",
            "C3:  0.855452865064695\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-B7Dh0PqKP"
      },
      "source": [
        "Step 5_3: update weight for all computer and check result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW0nbJyZmXOC",
        "outputId": "d9ed7759-2d3b-43f7-a16f-039a2e556b53"
      },
      "source": [
        "avg = (np.array(c1.get_weights()) + np.array(c2.get_weights()) + np.array(c3.get_weights()) + np.array(sv.get_weights())) / 4\n",
        "c1.set_weights(avg)\n",
        "c2.set_weights(avg)\n",
        "c3.set_weights(avg)\n",
        "sv.set_weights(avg)\n",
        "print(\"==============result Avg=============\")\n",
        "y_pred = sv.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Server: ', accuracy_score(y_pred1, y_pred))\n",
        "y_pred = c1.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Client 1: ', accuracy_score(y_pred1, y_pred))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============result Avg=============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server:  0.9478743068391867\n",
            "Client 1:  0.9478743068391867\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxiI-1WDQHHD"
      },
      "source": [
        "Step 6: Reuslt when train one computer which full dataset train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvizyRwRphu9",
        "outputId": "60cd6cf1-eb70-4f3c-e67b-1edab5f20c67"
      },
      "source": [
        "x = np.concatenate((trainX, trainX1, trainX2, trainX3, trainX4, trainX5, trainX6, trainX7), axis=0)\n",
        "y = np.concatenate((trainY, trainY1, trainY2, trainY3, trainY4, trainY5, trainY6, trainY7), axis=0)\n",
        "start = timeit.default_timer()\n",
        "model = define_model()\n",
        "model.fit(x, y, epochs=70, batch_size=512, validation_data=(valX, valY), verbose=0)\n",
        "print(\"==============Result with one computer=============\")\n",
        "y_pred = model.predict(test_X)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred1 = np.argmax(test_Y, axis=-1)\n",
        "print('Result: ', accuracy_score(y_pred1, y_pred))\n",
        "end = timeit.default_timer()\n",
        "print('time train full: {}'.format(end-start))\n",
        "print(\"========================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============Result with one computer=============\n",
            "Result:  0.9855822550831793\n",
            "time train full: 46.852277148999974\n",
            "========================================\n"
          ]
        }
      ]
    }
  ]
}